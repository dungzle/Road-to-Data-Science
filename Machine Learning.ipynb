{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/ What is Machine Learning?\n",
    "\n",
    "In traditional programming, a computer need to be told exactly what to do (explicit instructions: if you see X, then do Y). We configure a machine to accept our input and produce an output based on the algorithm (input = command, output = predetermined response). When problem get trickier and we can't explicitly instruct the computer what to do, we need machine learning\n",
    "\n",
    "Machine Learning is similar to how human learn: we give the computer the data and tools it needs to study and solve the problem without being told what to do. The computer has ability to adapt, evolve, and learn. \n",
    "\n",
    "**Machine learning is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II/ How Machine Learning works?\n",
    "\n",
    "UC Berkeley breaks out the learning system of a machine learning algorithm into three main parts.\n",
    "1. **A Decision Process**: In general, machine learning algorithms are used to make a prediction or classification. Based on some input data, which can be labelled or unlabeled, your algorithm will produce an estimate about a pattern in the data.\n",
    "\n",
    "\n",
    "2. **An Error Function**: An error function serves to evaluate the prediction of the model. If there are known examples, an error function can make a comparison to assess the accuracy of the model.\n",
    "\n",
    "\n",
    "3. **An Model Optimization Process**: If the model can fit better to the data points in the training set, then weights are adjusted to reduce the discrepancy between the known example and the model estimate. The algorithm will repeat this evaluate and optimize process, updating weights autonomously until a threshold of accuracy has been met.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III/ Different Ways a Machine learns:\n",
    "1. **Supervised Learning** (we teach the computer how to do sth)\n",
    "\n",
    "    Supervised Learning is defined by its use of *labeled datasets* to train algorithms that to classify data or predict outcomes accurately. As input data is fed into the model, it adjusts its weights until the model has been fitted appropriately. This occurs as part of the cross validation process to ensure that the model avoids overfitting or underfitting. Some methods used in supervised learning include neural networks, naïve bayes, linear regression, logistic regression, decision tree/random forest, support vector machine (SVM), and more.\n",
    "\n",
    "\n",
    "2. **Unsupervised Learning** (we let the computer learn by itself)\n",
    "\n",
    "    Unsupervised Learning uses machine learning algorithms to analyze and cluster *unlabeled datasets*. These algorithms discover hidden patterns or data groupings without the need for human intervention. Some algorithms used in unsupervised learning include neural networks, k-means clustering, probabilistic clustering methods, and more.\n",
    "        \n",
    "\n",
    "3. **Semi-supervised learning** offers a happy medium between supervised and unsupervised learning. During training, it uses a smaller labeled data set to guide classification and feature extraction from a larger, unlabeled data set. Semi-supervised learning can solve the problem of having not enough labeled data (or not being able to afford to label enough data) to train a supervised learning algorithm. \n",
    "\n",
    "\n",
    "4. **Reinforcement learning** is a behavioral machine learning model that is similar to supervised learning, but the algorithm isn’t trained using sample data. This model learns as it goes by using trial and error. A sequence of successful outcomes will be reinforced to develop the best recommendation or policy for a given problem.\n",
    "\n",
    "Source: https://www.ibm.com/cloud/learn/machine-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV/ Supervised Learning\n",
    "\n",
    "There are two types of Supervised Learning:\n",
    "    1. Regression\n",
    "    2. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=F6GSRDoB-Cg&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by feeding the training data to a Learning Algorithm. Then, that algorithm will output a function called hypothesis $h_{\\theta}(x)$. This hypothesis will map the new data to a predicted value:\n",
    "$$h_{\\theta}(x^{i}) = {\\theta_0} + {\\theta_1}{x_1} + ... + {\\theta_n}{x_n}$$\n",
    "\n",
    "How to choose those parameters $\\theta_i$ so that $h_{\\theta}(x)$ $\\approx$ y for given (x,y)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis:**\n",
    "$$h_{\\theta}(x^{i}) = {\\theta_0} + {\\theta_1}{x_1} + ... + {\\theta_n}{x_n}$$\n",
    "\n",
    "**Parameters**\n",
    "$$\\theta = vector [{\\theta_0}, {\\theta_1}, ... , {\\theta_n}]$$\n",
    "\n",
    "**Cost Function / Error function:**\n",
    "$$ J({\\theta}) = \\frac{1}{2m} \\sum_{i=1}^{m}(h_{\\theta}(x^{i}) - y^{i})^2$$\n",
    "\n",
    "**Goal: Minimize $J({\\theta_0},{\\theta_1},..,{\\theta_n})$**\n",
    "\n",
    "*Notation:*\n",
    "- *m = number of training examples*\n",
    "- *($x^{i}, y^{i}$) = $i^{th}$ example*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two ways to find the mininum of the cost function $J({\\theta})$:\n",
    "1. **Gradient descent** = an optimization algorithm for finding a local minimum of a differentiable function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient.\n",
    "2. **Normal equation** = linear algebra approach using matrix multiplication to find the optimal parameters $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Gradient Descent:**\n",
    "\n",
    "We simultaneously update ${\\theta_j}$ until J(${\\theta_j}$) converge to a local minimum\n",
    "\n",
    "    repeat until convergence (for j = 1, .., n) {\n",
    "$${\\theta_j} := {\\theta_j} - \\alpha*\\frac{\\partial}{\\partial {\\theta_j}}J({\\theta})$$\n",
    "    \n",
    "    }\n",
    "    \n",
    "where:\n",
    "- $\\alpha$ = learning rate\n",
    "- $\\frac{\\partial}{\\partial {\\theta_j}}J({\\theta})$ = partial derivative of cost function J with respect to $\\theta_j$ = slope of tangent line\n",
    "\n",
    "<img src=\"gradient_descent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "\n",
    "- **Feature Scaling**: make sure features are on a similar scale, which help gradient descent to converge more quickly \n",
    "\n",
    "    We should get every feature into approximately -1 $\\leq x_i \\leq 1$ range. \n",
    "    \n",
    "    We can do that by applying *Mean Normalization*: \n",
    "$$x_i = \\frac{{x_i} - {\\mu}_i}{s_i}$$\n",
    "     where: $s_i$ = range of $x_i$ in training set\n",
    "     \n",
    "     \n",
    "- **Making sure gradient descent is working correctly**: $J(\\theta)$ should decrease after every iteration\n",
    "  \n",
    "   If gradient descent is not working, use smaller $\\alpha$. \n",
    "   \n",
    "   - If $\\alpha$ is too small, gradient descent can be slow to converge.\n",
    "   \n",
    "   - If $\\alpha$ is too large, $J(\\theta)$ may not converge or not decrease on every iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Normal Equation:**\n",
    "\n",
    "Hypothesis\n",
    "$$h_{\\theta}(x^{i}) = {\\theta_0} + {\\theta_1}{x_1} + ... + {\\theta_n}{x_n}$$\n",
    "\n",
    "Let define $x_0 = 1$, then we have:\n",
    "$$h_{\\theta}(x^{i}) = {\\theta_0}{x_0} + {\\theta_1}{x_1} + ... + {\\theta_n}{x_n}$$\n",
    "\n",
    "$$ X = [{x_0},{x_1},..,{x_n}]^T$$\n",
    "\n",
    "We can calculate the optimal $\\theta$ by using below Normal Equation:\n",
    "$$\\theta = ({{X^T}X})^{-1}{X^T}y$$\n",
    "\n",
    "*Note: If ${X^T}X$ is non-invertible, try to remove redundant features or use regularization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to use Gradient Descent or Normal Equation?**\n",
    "\n",
    "Only choose Normal Equation if data is small\n",
    "\n",
    "| **Gradient Descent** | **Normal Equation** |\n",
    "| :-: | :-: |\n",
    "| Need to choose $\\alpha$ | Don't need to choose $\\alpha$ |\n",
    "| Need many iterations | Don't need many iterations |\n",
    "| Works well when n is large | Slow if n is large (because compute $({{X^T}X})^{-1}$ is $O(n^3)$|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
